{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    # lowercase all tweets\n",
    "    tweets = [[tweet[0].lower(), tweet[1]] for tweet in tweets]\n",
    "\n",
    "    # replace all users by @someuser\n",
    "    for i in range(0, len(tweets)):\n",
    "        tweets[i][0] = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z_]+[A-Za-z0-9_]+)',\n",
    "                              '@someuser',\n",
    "                              tweets[i][0])\n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible further preprocessing\n",
    "\n",
    "- Several emoticons are replaced by the tokens <smile>, <sadface>, <lolface> or <neutralface>. (There are some libraries for this)\n",
    "- e.g. “sooooo” is replaced by “soo”\n",
    "- remove the hash in front of the hastags and convert them to normal words\n",
    "- Better splitting of the tweets when calculating document vectors (now only split a spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(filename):\n",
    "    tweets = []\n",
    "    \n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            assert len(row) == 4\n",
    "            tweets.append([row[1], row[3].split(':')[0]])\n",
    "    \n",
    "    return preprocess_tweets(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = create_datasets('2018-Valence-oc-En-train.txt')\n",
    "dev_set = create_datasets('2018-Valence-oc-En-dev.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_vector(document, word_vectors):\n",
    "    document = document.split()\n",
    "    num_words = 0\n",
    "\n",
    "    average_vector = np.zeros(50)\n",
    "    for word in document:\n",
    "        if word in word_vectors:\n",
    "            average_vector += word_vectors[word]\n",
    "            num_words += 1\n",
    "\n",
    "    if num_words != 0:\n",
    "        average_vector /= num_words\n",
    "\n",
    "        return average_vector\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_set, word_vectors):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, len(data_set)):\n",
    "        document_vector = calculate_document_vector(data_set[i][0], word_vectors) \n",
    "        if document_vector is not None:\n",
    "            features.append(document_vector)\n",
    "            labels.append(data_set[i][1])\n",
    "\n",
    "    return np.asarray(features), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, test_set):\n",
    "    word_vectors = gensim.models.KeyedVectors.load_word2vec_format(\"glove.50d\", binary=False)\n",
    "    \n",
    "    train_X, train_Y = extract_features(train_set, word_vectors)\n",
    "    test_X, test_Y = extract_features(test_set, word_vectors)\n",
    "    \n",
    "    #assert not (np.all(np.isfinite(train_X)))\n",
    "    #assert not (np.all(np.isfinite(test_X)))\n",
    "    assert not (np.any(np.isnan(train_X)))\n",
    "    assert not (np.any(np.isnan(test_X)))\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    \n",
    "    model = LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=10)\n",
    "\n",
    "    model.fit(train_X, train_Y)\n",
    "\n",
    "    print(\"Start prediction\")\n",
    "    \n",
    "    train_predict = model.predict(train_X)\n",
    "    train_acc = np.mean(train_predict == train_Y)\n",
    "    print(\"Train set accuracy\", train_acc)\n",
    "\n",
    "    test_predict = model.predict(test_X)\n",
    "    test_acc = np.mean(test_predict == test_Y)\n",
    "    print(\"Test set accuracy\", test_acc)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\nStart prediction\nTrain set accuracy 0.398116438356\nTest set accuracy 0.266331658291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendaet/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:717: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "model = train_model(train_set, dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
